{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An initial step is to perform an evaluation of the notes with which we have worked. We evaluated the number of tokens, number of entities, their relationship for a correct interpretation of the tokens and to make an adequate approach.\n",
    "\n",
    "In the images [Frequency_tokens_entitiesref](./Frequency_tokens_entities.png) and [Relation_numtokens_entities](./Relation_numtokens_entities.png) you can see an example of some of the visualizations made for the initial understanding of the notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "from database import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "from utils import get_number_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_PATH = os.getenv(\"CORPUS_CLINICAL_PATH\")\n",
    "MODEL_PATH = os.getenv(\"MODEL_CLINICAL_PATH\")\n",
    "CORPUS_PATH_OUT = os.getenv(\"CORPUS_CLINICAL_FILTERED_PATH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = spacy.load(MODEL_PATH)\n",
    "\n",
    "ENTS = spacy.info(MODEL_PATH)['labels']['ner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ENTS))\n",
    "pprint(ENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = load_dataset(MODEL, CORPUS_PATH)\n",
    "print(\"Number of notes:\", len(docs))\n",
    "print(\"Number of tokens:\", get_number_tokens(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Covert from notes to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Span\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_doc(model, sent, span_list):\n",
    "    doc = model(sent.text)\n",
    "    new_ents = []\n",
    "\n",
    "    sent_start = sent.start\n",
    "\n",
    "    for span in span_list:\n",
    "        start =  span.start\n",
    "        end =  span.end \n",
    "        label = span.label_\n",
    "\n",
    "        if sent.start <= start < sent.end:\n",
    "            \n",
    "            entidad = Span(doc, start - sent_start, end - sent_start, label=label)\n",
    "            new_ents.append(entidad)\n",
    "\n",
    "    doc.ents = new_ents\n",
    "\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentencizer = MODEL.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_in_sentences(docs):\n",
    "    docs_sents =[]\n",
    "    for doc in tqdm(sentencizer.pipe(docs, batch_size=250), total=len(docs)):\n",
    "        for sent in doc.sents:\n",
    "            # sent == Span\n",
    "            span_start, span_end = sent.start, sent.end\n",
    "            span = doc[span_start:span_end]\n",
    "\n",
    "            docs_sents.append(gen_doc(MODEL, sent, span.ents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "from utils import get_tokens_ents\n",
    "from visualize import visualize_ent_scatter, visualize_distrib_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ents_counter = { ent : get_tokens_ents(docs, ent) for ent in ENTS }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in ents_counter:\n",
    "    tokens, ents = ents_counter[ent]\n",
    "    visualize_ent_scatter(tokens,ents, ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in ents_counter:\n",
    "    tokens, ents = ents_counter[ent]\n",
    "    visualize_distrib_outliers(tokens, ents, ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualize import box_plot\n",
    "\n",
    "tokens, ents = ents_counter[list(ents_counter.keys())[0]]\n",
    "box_plot(tokens, \"Number of Tokens\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter by number of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_good = [d for d in docs if get_number_tokens([d]) <= 250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of good docs:\", len(docs_good))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import calculate_and_categorize_entities, select_entity_results, save_datasets\n",
    "\n",
    "notes_with_1_ents, notes_with_morethan1_ents, notes_without_ents = calculate_and_categorize_entities(docs_good, ENTS)\n",
    "\n",
    "for label in ENTS:\n",
    "    ent_0, ent_1, ent_morethan1 = select_entity_results(docs_good, label, notes_with_1_ents, notes_with_morethan1_ents, notes_without_ents)\n",
    "    save_datasets({\"eq0_ents\":ent_0, \"eq1_ents\":ent_1, \"gt1_ents\":ent_morethan1}, CORPUS_PATH_OUT, label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_datasets\n",
    "\n",
    "dic_docs = load_datasets(MODEL, CORPUS_PATH_OUT, \"CANCER_CONCEPT\")\n",
    "print(len(dic_docs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
