{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-NER \n",
    "\n",
    "Schema used to perform GPT queries for the study entities. It is performed for each entity separately. \n",
    "\n",
    "For each of the entities, separate results have been obtained for grades with 0,1 and >1 entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import spacy\n",
    "from main import LargeLanguageModel, PARAMS, log_ner\n",
    "from costs import GPT3, GPT4\n",
    "from models import Message, Sample\n",
    "from database import load_promts\n",
    "from utils import load_datasets\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = os.getenv(\"MODEL_CLINICAL_PATH\")\n",
    "CORPUS_PATH = os.getenv(\"CORPUS_CLINICAL_PATH\")\n",
    "CORPUS_PATH_OUT = os.getenv(\"CORPUS_CLINICAL_FILTERED_PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTS = spacy.info(MODEL_PATH)['labels']['ner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ENTS[0]\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = spacy.load(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_dict = load_datasets(MODEL, CORPUS_PATH_OUT, label)\n",
    "docs_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_1ent = docs_dict[\"eq1_ents\"]\n",
    "len(docs_1ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_0ent = docs_dict[\"eq0_ents\"]\n",
    "len(docs_0ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_mt1ent = docs_dict[\"gt1_ents\"]\n",
    "len(docs_mt1ent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = load_promts(\"prompts-clinical.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(file_name):\n",
    "    ds_folder = \"ds\"\n",
    "    file_path = os.path.join(ds_folder, file_name)\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "file_name = \"good_values.json\"\n",
    "json_data = load_json(file_name)\n",
    "\n",
    "if json_data is not None:\n",
    "    print(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if json_data is not None and label in json_data:\n",
    "    values = json_data[label]\n",
    "    values = ', '.join(values)\n",
    "    print(label)\n",
    "    print(values)\n",
    "else:\n",
    "    print(f\"Not found '{label}' in JSON.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompts[1]\n",
    "behave =  prompt.msg\n",
    "behave[\"content\"] = behave[\"content\"].replace(\"$$\", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2 = prompts[2]\n",
    "behave2 = prompt2.msg\n",
    "behave2[\"content\"] = behave2[\"content\"].replace(\"$$\", label)\n",
    "behave2[\"content\"] = behave2[\"content\"].replace(\"&&\", values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llms = [ LargeLanguageModel(GPT3, **PARAMS), LargeLanguageModel(GPT4, **PARAMS)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import select_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_docs, other_docs = select_notes(docs_1ent, 5, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sample_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_docs = docs_0ent\n",
    "print(len(eval_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gptFormat import encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [Sample(user=Message(role=\"user\", content=doc.text),\n",
    "                  agent=Message(role=\"assistant\", content=encoder(doc, label)))\n",
    "           for doc in sample_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query openAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import SetEvalDocs, Promt\n",
    "from utils import eval_pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets_eval = [SetEvalDocs({\"name\": \"set_test_CANCER_CONCEPT_eq0\" , \"docs\": eval_docs})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "promts2test = [ Promt(behave=behave, name=\"Zero Shot we\"), Promt(behave=behave, name=\"5 Few-shot we\", samples=samples), Promt(behave=behave2, name=\"Zero Shot\"), Promt(behave=behave2, name=\"5 Few-shot\", samples=samples)]\n",
    "promts2test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = eval_pipline(llms, sets_eval, promts2test, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import dump_results\n",
    "label_eq0 = label + \"_eq0\"\n",
    "path = \"results/sample/\" + label_eq0 + \".json\"\n",
    "dump_results(results, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sample_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(other_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_docs = other_docs\n",
    "print(len(eval_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [Sample(user=Message(role=\"user\", content=doc.text),\n",
    "                  agent=Message(role=\"assistant\", content=encoder(doc, label)))\n",
    "           for doc in sample_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query openAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets_eval = [SetEvalDocs({\"name\": \"set_test_CANCER_CONCEPT_eq1\" , \"docs\": eval_docs})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "promts2test = [ Promt(behave=behave, name=\"Zero Shot we\"), Promt(behave=behave, name=\"5 Few-shot we\", samples=samples), Promt(behave=behave2, name=\"Zero Shot\"), Promt(behave=behave2, name=\"5 Few-shot\", samples=samples)]\n",
    "promts2test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = eval_pipline(llms, sets_eval, promts2test, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_eq1 = label + \"_eq1\"\n",
    "path = \"results/sample/\" + label_eq1 + \".json\"\n",
    "dump_results(results, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## >1 ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_docs, other_docs = select_notes(docs_mt1ent, 5, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sample_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(other_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_docs = other_docs\n",
    "print(len(eval_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [Sample(user=Message(role=\"user\", content=doc.text),\n",
    "                  agent=Message(role=\"assistant\", content=encoder(doc, label)))\n",
    "           for doc in sample_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query openAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets_eval = [SetEvalDocs({\"name\": \"set_test_CANCER_CONCEPT_gt1\" , \"docs\": eval_docs})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "promts2test = [ Promt(behave=behave, name=\"Zero Shot we\"), Promt(behave=behave, name=\"5 Few-shot we\", samples=samples), Promt(behave=behave2, name=\"Zero Shot\"), Promt(behave=behave2, name=\"5 Few-shot\", samples=samples)]\n",
    "promts2test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = eval_pipline(llms, sets_eval, promts2test, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_gt1 = label + \"_gt1\"\n",
    "path = \"results/sample/\" + label_gt1 + \".json\"\n",
    "dump_results(results, path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
